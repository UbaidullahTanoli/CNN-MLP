{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNEN+FDo5ocF4R9AHBhNV0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UbaidullahTanoli/CNN-MLP/blob/main/CNN%2BMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U3VKBBRaZ-Ya"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "3qouqCdJZ_tc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d raddar/chest-xrays-indiana-university\n",
        "!unzip chest-xrays-indiana-university.zip -d /content/dataset/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KQAwrLo4Z_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import EfficientNet_B1_Weights\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "jd3NyR0QZ__W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Custom Dataset that Merges Two CSVs\n",
        "# ================================\n",
        "class PureCNNMergedDataset(Dataset):\n",
        "    def __init__(self, reports_csv, proj_csv, image_folder, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            reports_csv (str): Path to 'indiana_reports.csv', which includes the \"MeSH\" column for labels.\n",
        "            proj_csv (str): Path to 'indiana_projections.csv', which maps uid to image filename.\n",
        "            image_folder (str): Directory containing the image files.\n",
        "            transform (callable, optional): Transform to apply on images.\n",
        "        \"\"\"\n",
        "        # Load both CSVs\n",
        "        self.reports_df = pd.read_csv(reports_csv)\n",
        "        self.proj_df = pd.read_csv(proj_csv)\n",
        "        # Merge on 'uid'\n",
        "        self.data = pd.merge(self.reports_df, self.proj_df, on='uid')\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        # Store all labels for class balancing\n",
        "        self.labels = []\n",
        "        for idx in range(len(self.data)):\n",
        "            row = self.data.iloc[idx]\n",
        "            mesh_val = str(row['MeSH']).strip().lower()\n",
        "            label = 0 if mesh_val == 'normal' else 1\n",
        "            self.labels.append(label)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        # Get the image filename from the projections CSV merged with reports CSV\n",
        "        filename = row['filename']  # e.g., \"1_IM-0001-4001.dcm.png\"\n",
        "        img_path = os.path.join(self.image_folder, filename)\n",
        "\n",
        "        # Load the image and convert to RGB if needed\n",
        "        pil_image = Image.open(img_path)\n",
        "        if pil_image.mode != 'RGB':\n",
        "            pil_image = pil_image.convert('RGB')\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(pil_image)\n",
        "        else:\n",
        "            image_tensor = transforms.ToTensor()(pil_image)\n",
        "\n",
        "        # Derive binary label from the \"MeSH\" column in the reports CSV\n",
        "        # e.g., if MeSH equals \"normal\" (case-insensitive), label = 0; else, label = 1.\n",
        "        mesh_val = str(row['MeSH']).strip().lower()\n",
        "        label = 0 if mesh_val == 'normal' else 1\n",
        "\n",
        "        return image_tensor, label\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        \"\"\"\n",
        "        Returns the count of each class in the dataset.\n",
        "        \"\"\"\n",
        "        class_counts = {}\n",
        "        for label in self.labels:\n",
        "            if label in class_counts:\n",
        "                class_counts[label] += 1\n",
        "            else:\n",
        "                class_counts[label] = 1\n",
        "        return class_counts"
      ],
      "metadata": {
        "id": "fnCuiO5maAC-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Transforms with Augmentation\n",
        "# ================================\n",
        "# Define transforms for training with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transforms for validation (no augmentation)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "aOrIusZJbI4j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Enhanced CNN Model with MLP, Batch Normalization\n",
        "# ================================\n",
        "class PureCNNModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(PureCNNModel, self).__init__()\n",
        "\n",
        "        # Load EfficientNet-B1 pretrained on ImageNet\n",
        "        self.backbone = models.efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Get the input features of the classifier (EfficientNet-B1 typically uses 1280)\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "\n",
        "        # Replace the final classifier with an MLP including BatchNorm\n",
        "        self.backbone.classifier[1] = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Freeze all backbone feature layers initially\n",
        "        for param in self.backbone.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def unfreeze_last_block(self):\n",
        "        \"\"\"\n",
        "        Unfreeze the last block of the EfficientNet backbone for fine-tuning\n",
        "        \"\"\"\n",
        "        for param in self.backbone.features[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_last_n_blocks(self, n=2):\n",
        "        \"\"\"\n",
        "        Unfreeze the last n blocks of the EfficientNet backbone\n",
        "        \"\"\"\n",
        "        for i in range(n):\n",
        "            block_idx = len(self.backbone.features) - 1 - i\n",
        "            if block_idx >= 0:\n",
        "                for param in self.backbone.features[block_idx].parameters():\n",
        "                    param.requires_grad = True"
      ],
      "metadata": {
        "id": "ZFtmqJoybHPK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training and Evaluation Functions\n",
        "# ================================\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds_all, labels_all = [], []\n",
        "    probs_all = []  # Added for ROC AUC calculation\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Get probabilities for positive class (needed for AUC)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        preds_all.extend(preds.cpu().numpy())\n",
        "        labels_all.extend(labels.cpu().numpy())\n",
        "        probs_all.extend(probs.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(labels_all, preds_all)\n",
        "    epoch_prec = precision_score(labels_all, preds_all, zero_division=0)\n",
        "    epoch_rec = recall_score(labels_all, preds_all, zero_division=0)\n",
        "    epoch_f1 = f1_score(labels_all, preds_all, zero_division=0)\n",
        "\n",
        "    # Add AUC metrics\n",
        "    try:\n",
        "        epoch_roc_auc = roc_auc_score(labels_all, probs_all)\n",
        "        epoch_pr_auc = average_precision_score(labels_all, probs_all)\n",
        "    except:\n",
        "        # Handle edge cases where all labels might be from the same class\n",
        "        epoch_roc_auc = 0.0\n",
        "        epoch_pr_auc = 0.0\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_prec, epoch_rec, epoch_f1, epoch_roc_auc, epoch_pr_auc\n",
        "\n",
        "def eval_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds_all, labels_all = [], []\n",
        "    probs_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Get class probabilities for positive class (needed for AUC)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            preds_all.extend(preds.cpu().numpy())\n",
        "            labels_all.extend(labels.cpu().numpy())\n",
        "            probs_all.extend(probs.detach().cpu().numpy())  # Fixed: changed label to probs\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = accuracy_score(labels_all, preds_all)\n",
        "    epoch_prec = precision_score(labels_all, preds_all, zero_division=0)\n",
        "    epoch_rec = recall_score(labels_all, preds_all, zero_division=0)\n",
        "    epoch_f1 = f1_score(labels_all, preds_all, zero_division=0)\n",
        "\n",
        "    # Add AUC metrics\n",
        "    try:\n",
        "        epoch_roc_auc = roc_auc_score(labels_all, probs_all)\n",
        "        epoch_pr_auc = average_precision_score(labels_all, probs_all)\n",
        "    except:\n",
        "        # Handle edge cases where all labels might be from the same class\n",
        "        epoch_roc_auc = 0.0\n",
        "        epoch_pr_auc = 0.0\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_prec, epoch_rec, epoch_f1, epoch_roc_auc, epoch_pr_auc"
      ],
      "metadata": {
        "id": "BWa9rXwPa1u1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Cross-Validation Implementation\n",
        "# ================================\n",
        "def cross_validate(reports_csv, proj_csv, image_folder, num_epochs=40, k=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Load the full dataset\n",
        "    full_dataset = PureCNNMergedDataset(reports_csv, proj_csv, image_folder, transform=None)\n",
        "\n",
        "    # Check class distribution\n",
        "    class_distribution = full_dataset.get_class_distribution()\n",
        "    print(f\"Class distribution: {class_distribution}\")\n",
        "\n",
        "    # Calculate class weights for weighted loss\n",
        "    num_samples = len(full_dataset)\n",
        "    class_weights = torch.FloatTensor([num_samples / (len(class_distribution) * count)\n",
        "                                       for count in class_distribution.values()])\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # For each fold\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(full_dataset)))):\n",
        "        print(f\"\\nTraining Fold {fold+1}/{k}\")\n",
        "\n",
        "        # Create datasets with appropriate transforms\n",
        "        train_dataset = PureCNNMergedDataset(reports_csv, proj_csv, image_folder, transform=train_transform)\n",
        "        val_dataset = PureCNNMergedDataset(reports_csv, proj_csv, image_folder, transform=val_transform)\n",
        "\n",
        "        # Create subset samplers\n",
        "        train_subsampler = SubsetRandomSampler(train_idx)\n",
        "        val_subsampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, sampler=train_subsampler, num_workers=2)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8, sampler=val_subsampler, num_workers=2)\n",
        "\n",
        "        # Initialize the model\n",
        "        model = PureCNNModel(num_classes=2).to(device)\n",
        "\n",
        "        # Create criterion with class weights\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "        # Initial training phase: Only train the MLP head\n",
        "        optimizer = optim.AdamW(model.backbone.classifier.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1,\n",
        "                                                        patience=3)\n",
        "\n",
        "        # First phase: Train only the MLP head for 15 epochs\n",
        "        print(\"Phase 1: Training only the MLP head...\")\n",
        "        for epoch in range(15):\n",
        "            train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc, train_pr_auc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device\n",
        "            )\n",
        "            val_loss, val_acc, val_prec, val_rec, val_f1, val_roc_auc, val_pr_auc = eval_epoch(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/15\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f} | ROC AUC: {train_roc_auc:.4f} | PR AUC: {train_pr_auc:.4f}\")\n",
        "            print(f\"  Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f} | ROC AUC: {val_roc_auc:.4f} | PR AUC: {val_pr_auc:.4f}\")\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "            current_lr = scheduler.get_last_lr()\n",
        "            print(f\"Epoch {epoch+1}/15 - Current learning rate: {current_lr}\")\n",
        "\n",
        "        # Second phase: Unfreeze the last block and train with a lower learning rate\n",
        "        print(\"\\nPhase 2: Unfreezing last block and fine-tuning...\")\n",
        "        model.unfreeze_last_block()\n",
        "\n",
        "        # Create new optimizer with different learning rates for different parts\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': model.backbone.features[-1].parameters(), 'lr': 1e-4},\n",
        "            {'params': model.backbone.classifier.parameters(), 'lr': 1e-3}\n",
        "        ], weight_decay=0.01)\n",
        "\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1,\n",
        "                                                        patience=3)\n",
        "\n",
        "        best_val_f1 = 0\n",
        "        for epoch in range(num_epochs - 15):\n",
        "            train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc, train_pr_auc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device\n",
        "            )\n",
        "            val_loss, val_acc, val_prec, val_rec, val_f1, val_roc_auc, val_pr_auc = eval_epoch(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs-15}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f} | ROC AUC: {train_roc_auc:.4f} | PR AUC: {train_pr_auc:.4f}\")\n",
        "            print(f\"  Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f} | ROC AUC: {val_roc_auc:.4f} | PR AUC: {val_pr_auc:.4f}\")\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            current_lr = scheduler.get_last_lr()\n",
        "            print(f\"Epoch {epoch+1}/15 - Current learning rate: {current_lr}\")\n",
        "\n",
        "            # Save the best model for this fold\n",
        "            if val_f1 > best_val_f1:\n",
        "                best_val_f1 = val_f1\n",
        "                torch.save(model.state_dict(), f\"best_model_fold_{fold}.pt\")\n",
        "                print(f\"  Saved new best model with validation F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Load the best model for final evaluation\n",
        "        model.load_state_dict(torch.load(f\"best_model_fold_{fold}.pt\"))\n",
        "        _, fold_acc, fold_prec, fold_rec, fold_f1, fold_roc_auc, fold_pr_auc = eval_epoch(model, val_loader, criterion, device)\n",
        "        fold_results = {\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': fold_acc,\n",
        "            'precision': fold_prec,\n",
        "            'recall': fold_rec,\n",
        "            'f1': fold_f1,\n",
        "            'roc_auc': fold_roc_auc,\n",
        "            'pr_auc': fold_pr_auc\n",
        "        }\n",
        "\n",
        "        all_results.append(fold_results)\n",
        "\n",
        "        print(f\"\\nFold {fold+1} Results:\")\n",
        "        for metric, value in fold_results.items():\n",
        "            if metric != 'fold':\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Calculate and print average metrics across all folds\n",
        "    avg_metrics = {metric: sum(result[metric] for result in all_results) / k\n",
        "                for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc']}\n",
        "\n",
        "    print(\"\\nAverage Results Across All Folds:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    return all_results, avg_metrics"
      ],
      "metadata": {
        "id": "V2VuVsWhajEf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Standard Training Function (for comparison)\n",
        "# ================================\n",
        "def train_model(reports_csv, proj_csv, image_folder, num_epochs=40):\n",
        "    # Create the merged dataset with train augmentation\n",
        "    train_dataset = PureCNNMergedDataset(reports_csv, proj_csv, image_folder, transform=train_transform)\n",
        "\n",
        "    # Check class distribution for weighted loss\n",
        "    class_distribution = train_dataset.get_class_distribution()\n",
        "    print(f\"Class distribution: {class_distribution}\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    num_samples = len(train_dataset)\n",
        "    class_weights = torch.FloatTensor([num_samples / (len(class_distribution) * count)\n",
        "                                      for count in class_distribution.values()])\n",
        "\n",
        "    # Split the dataset (80% training, 20% testing)\n",
        "    total_size = len(train_dataset)\n",
        "    train_size = int(0.8 * total_size)\n",
        "    test_size = total_size - train_size\n",
        "    train_subset, test_subset = random_split(train_dataset, [train_size, test_size])\n",
        "\n",
        "    # Create test dataset with test transform\n",
        "    test_dataset = PureCNNMergedDataset(reports_csv, proj_csv, image_folder, transform=val_transform)\n",
        "\n",
        "    # We need to use the same indices for the test set\n",
        "    test_indices = test_subset.indices\n",
        "    test_subset = torch.utils.data.Subset(test_dataset, test_indices)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_subset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Set device (GPU if available)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = PureCNNModel(num_classes=2).to(device)\n",
        "\n",
        "    # Define loss function with class weights\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    # First phase: Train only the classifier\n",
        "    print(\"Phase 1: Training only the MLP head...\")\n",
        "    optimizer = optim.AdamW(model.backbone.classifier.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "    for epoch in range(15):\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc, train_pr_auc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        test_loss, test_acc, test_prec, test_rec, test_f1, test_roc_auc, test_pr_auc = eval_epoch(\n",
        "            model, test_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/15\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f} | ROC AUC: {train_roc_auc:.4f} | PR AUC: {train_pr_auc:.4f}\")\n",
        "        print(f\"  Test  Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f} | ROC AUC: {test_roc_auc:.4f} | PR AUC: {test_pr_auc:.4f}\")\n",
        "\n",
        "        scheduler.step(test_loss)\n",
        "        current_lr = scheduler.get_last_lr()\n",
        "        print(f\"Epoch {epoch+1} - Current learning rate: {current_lr}\")\n",
        "\n",
        "\n",
        "    # Second phase: Unfreeze the last block and continue training\n",
        "    print(\"\\nPhase 2: Unfreezing last block and fine-tuning...\")\n",
        "    model.unfreeze_last_block()\n",
        "\n",
        "    # Create optimizer with different learning rates\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': model.backbone.features[-1].parameters(), 'lr': 1e-4},\n",
        "        {'params': model.backbone.classifier.parameters(), 'lr': 1e-3}\n",
        "    ], weight_decay=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "    best_test_f1 = 0\n",
        "    for epoch in range(num_epochs - 15):\n",
        "        train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc, train_pr_auc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        test_loss, test_acc, test_prec, test_rec, test_f1, test_roc_auc, test_pr_auc = eval_epoch(\n",
        "            model, test_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs-15}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f} | ROC AUC: {train_roc_auc:.4f} | PR AUC: {train_pr_auc:.4f}\")\n",
        "        print(f\"  Test  Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f} | ROC AUC: {test_roc_auc:.4f} | PR AUC: {test_pr_auc:.4f}\")\n",
        "\n",
        "        scheduler.step(test_loss)\n",
        "        current_lr = scheduler.get_last_lr()\n",
        "        print(f\"Epoch {epoch+1} - Current learning rate: {current_lr}\")\n",
        "\n",
        "\n",
        "        # Save best model\n",
        "        if test_f1 > best_test_f1:\n",
        "            best_test_f1 = test_f1\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            print(f\"  Saved new best model with test F1: {test_f1:.4f}\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    final_loss, final_acc, final_prec, final_rec, final_f1, final_roc_auc, final_pr_auc = eval_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\"\\nFinal Model Performance:\")\n",
        "    print(f\"  Accuracy: {final_acc:.4f}\")\n",
        "    print(f\"  Precision: {final_prec:.4f}\")\n",
        "    print(f\"  Recall: {final_rec:.4f}\")\n",
        "    print(f\"  F1 Score: {final_f1:.4f}\")\n",
        "    print(f\"  ROC AUC: {final_roc_auc:.4f}\")\n",
        "    print(f\"  PR AUC: {final_pr_auc:.4f}\")\n",
        "\n",
        "    return model, {\n",
        "        \"accuracy\": final_acc,\n",
        "        \"precision\": final_prec,\n",
        "        \"recall\": final_rec,\n",
        "        \"f1\": final_f1,\n",
        "        \"roc_auc\": final_roc_auc,\n",
        "        \"pr_auc\": final_pr_auc\n",
        "    }"
      ],
      "metadata": {
        "id": "J5WXfPwfaUNQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Main Function\n",
        "# ================================\n",
        "def main():\n",
        "    # Paths to the CSV files and image folder\n",
        "    reports_csv = '/content/dataset/indiana_reports.csv'\n",
        "    proj_csv = '/content/dataset/indiana_projections.csv'\n",
        "    image_folder = '/content/dataset/images/images_normalized'\n",
        "\n",
        "    # Decide whether to use cross-validation or standard training\n",
        "    use_cross_validation = True\n",
        "\n",
        "    if use_cross_validation:\n",
        "        print(\"Running 5-fold cross-validation...\")\n",
        "        results, avg_metrics = cross_validate(reports_csv, proj_csv, image_folder, num_epochs=40, k=5)\n",
        "    else:\n",
        "        print(\"Running standard training with 80/20 split...\")\n",
        "        model, metrics = train_model(reports_csv, proj_csv, image_folder, num_epochs=40)"
      ],
      "metadata": {
        "id": "9btQxDz-aGNh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I8jckDQAaCqP",
        "outputId": "ee92a041-ee90-404e-a057-e65501a912b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 5-fold cross-validation...\n",
            "Class distribution: {np.int64(0): 2695, np.int64(1): 4771}\n",
            "\n",
            "Training Fold 1/5\n",
            "Phase 1: Training only the MLP head...\n",
            "Epoch 1/15\n",
            "  Train Loss: 0.5545 | Acc: 0.5881 | F1: 0.6613 | ROC AUC: 0.6101 | PR AUC: 0.7294\n",
            "  Val   Loss: 0.1300 | Acc: 0.6339 | F1: 0.6887 | ROC AUC: 0.6723 | PR AUC: 0.7574\n",
            "Epoch 1/15 - Current learning rate: [0.001]\n",
            "Epoch 2/15\n",
            "  Train Loss: 0.5325 | Acc: 0.6202 | F1: 0.6919 | ROC AUC: 0.6437 | PR AUC: 0.7574\n",
            "  Val   Loss: 0.1286 | Acc: 0.6118 | F1: 0.6393 | ROC AUC: 0.6820 | PR AUC: 0.7582\n",
            "Epoch 2/15 - Current learning rate: [0.001]\n",
            "Epoch 3/15\n",
            "  Train Loss: 0.5296 | Acc: 0.6114 | F1: 0.6796 | ROC AUC: 0.6532 | PR AUC: 0.7682\n",
            "  Val   Loss: 0.1304 | Acc: 0.6432 | F1: 0.6987 | ROC AUC: 0.6788 | PR AUC: 0.7525\n",
            "Epoch 3/15 - Current learning rate: [0.001]\n",
            "Epoch 4/15\n",
            "  Train Loss: 0.5208 | Acc: 0.6351 | F1: 0.7031 | ROC AUC: 0.6661 | PR AUC: 0.7785\n",
            "  Val   Loss: 0.1283 | Acc: 0.6560 | F1: 0.7191 | ROC AUC: 0.6923 | PR AUC: 0.7661\n",
            "Epoch 4/15 - Current learning rate: [0.001]\n",
            "Epoch 5/15\n",
            "  Train Loss: 0.5181 | Acc: 0.6289 | F1: 0.6965 | ROC AUC: 0.6715 | PR AUC: 0.7858\n",
            "  Val   Loss: 0.1261 | Acc: 0.6620 | F1: 0.7158 | ROC AUC: 0.7026 | PR AUC: 0.7756\n",
            "Epoch 5/15 - Current learning rate: [0.001]\n",
            "Epoch 6/15\n",
            "  Train Loss: 0.5183 | Acc: 0.6341 | F1: 0.7014 | ROC AUC: 0.6727 | PR AUC: 0.7821\n",
            "  Val   Loss: 0.1284 | Acc: 0.5843 | F1: 0.5778 | ROC AUC: 0.6956 | PR AUC: 0.7677\n",
            "Epoch 6/15 - Current learning rate: [0.001]\n",
            "Epoch 7/15\n",
            "  Train Loss: 0.5168 | Acc: 0.6386 | F1: 0.7023 | ROC AUC: 0.6762 | PR AUC: 0.7887\n",
            "  Val   Loss: 0.1293 | Acc: 0.5609 | F1: 0.5198 | ROC AUC: 0.7018 | PR AUC: 0.7783\n",
            "Epoch 7/15 - Current learning rate: [0.001]\n",
            "Epoch 8/15\n",
            "  Train Loss: 0.5136 | Acc: 0.6407 | F1: 0.7038 | ROC AUC: 0.6820 | PR AUC: 0.7908\n",
            "  Val   Loss: 0.1250 | Acc: 0.6439 | F1: 0.6787 | ROC AUC: 0.7076 | PR AUC: 0.7813\n",
            "Epoch 8/15 - Current learning rate: [0.001]\n",
            "Epoch 9/15\n",
            "  Train Loss: 0.5135 | Acc: 0.6390 | F1: 0.7029 | ROC AUC: 0.6792 | PR AUC: 0.7898\n",
            "  Val   Loss: 0.1269 | Acc: 0.6446 | F1: 0.6856 | ROC AUC: 0.6972 | PR AUC: 0.7698\n",
            "Epoch 9/15 - Current learning rate: [0.001]\n",
            "Epoch 10/15\n",
            "  Train Loss: 0.5062 | Acc: 0.6383 | F1: 0.7019 | ROC AUC: 0.6924 | PR AUC: 0.8033\n",
            "  Val   Loss: 0.1246 | Acc: 0.6426 | F1: 0.6658 | ROC AUC: 0.7161 | PR AUC: 0.7843\n",
            "Epoch 10/15 - Current learning rate: [0.001]\n",
            "Epoch 11/15\n",
            "  Train Loss: 0.5080 | Acc: 0.6469 | F1: 0.7084 | ROC AUC: 0.6933 | PR AUC: 0.7937\n",
            "  Val   Loss: 0.1260 | Acc: 0.6379 | F1: 0.6731 | ROC AUC: 0.7057 | PR AUC: 0.7708\n",
            "Epoch 11/15 - Current learning rate: [0.001]\n",
            "Epoch 12/15\n",
            "  Train Loss: 0.5139 | Acc: 0.6356 | F1: 0.7018 | ROC AUC: 0.6803 | PR AUC: 0.7919\n",
            "  Val   Loss: 0.1255 | Acc: 0.6720 | F1: 0.7253 | ROC AUC: 0.7145 | PR AUC: 0.7834\n",
            "Epoch 12/15 - Current learning rate: [0.001]\n",
            "Epoch 13/15\n",
            "  Train Loss: 0.5101 | Acc: 0.6440 | F1: 0.7068 | ROC AUC: 0.6869 | PR AUC: 0.7927\n",
            "  Val   Loss: 0.1267 | Acc: 0.6714 | F1: 0.7336 | ROC AUC: 0.7105 | PR AUC: 0.7801\n",
            "Epoch 13/15 - Current learning rate: [0.001]\n",
            "Epoch 14/15\n",
            "  Train Loss: 0.5105 | Acc: 0.6499 | F1: 0.7132 | ROC AUC: 0.6868 | PR AUC: 0.7941\n",
            "  Val   Loss: 0.1267 | Acc: 0.6673 | F1: 0.7256 | ROC AUC: 0.7096 | PR AUC: 0.7755\n",
            "Epoch 14/15 - Current learning rate: [0.0001]\n",
            "Epoch 15/15\n",
            "  Train Loss: 0.5077 | Acc: 0.6452 | F1: 0.7121 | ROC AUC: 0.6888 | PR AUC: 0.7965\n",
            "  Val   Loss: 0.1259 | Acc: 0.6506 | F1: 0.6893 | ROC AUC: 0.7052 | PR AUC: 0.7744\n",
            "Epoch 15/15 - Current learning rate: [0.0001]\n",
            "\n",
            "Phase 2: Unfreezing last block and fine-tuning...\n",
            "Epoch 1/25\n",
            "  Train Loss: 0.5039 | Acc: 0.6517 | F1: 0.7135 | ROC AUC: 0.6995 | PR AUC: 0.8062\n",
            "  Val   Loss: 0.1271 | Acc: 0.6191 | F1: 0.6234 | ROC AUC: 0.7064 | PR AUC: 0.7750\n",
            "Epoch 1/15 - Current learning rate: [0.0001, 0.001]\n",
            "  Saved new best model with validation F1: 0.6234\n",
            "Epoch 2/25\n",
            "  Train Loss: 0.5107 | Acc: 0.6417 | F1: 0.7060 | ROC AUC: 0.6839 | PR AUC: 0.7949\n",
            "  Val   Loss: 0.1247 | Acc: 0.6787 | F1: 0.7315 | ROC AUC: 0.7233 | PR AUC: 0.7890\n",
            "Epoch 2/15 - Current learning rate: [0.0001, 0.001]\n",
            "  Saved new best model with validation F1: 0.7315\n",
            "Epoch 3/25\n",
            "  Train Loss: 0.5027 | Acc: 0.6490 | F1: 0.7122 | ROC AUC: 0.6980 | PR AUC: 0.8055\n",
            "  Val   Loss: 0.1239 | Acc: 0.6801 | F1: 0.7256 | ROC AUC: 0.7217 | PR AUC: 0.7862\n",
            "Epoch 3/15 - Current learning rate: [0.0001, 0.001]\n",
            "Epoch 4/25\n",
            "  Train Loss: 0.4962 | Acc: 0.6618 | F1: 0.7207 | ROC AUC: 0.7124 | PR AUC: 0.8124\n",
            "  Val   Loss: 0.1241 | Acc: 0.6888 | F1: 0.7446 | ROC AUC: 0.7286 | PR AUC: 0.7927\n",
            "Epoch 4/15 - Current learning rate: [0.0001, 0.001]\n",
            "  Saved new best model with validation F1: 0.7446\n",
            "Epoch 5/25\n",
            "  Train Loss: 0.4971 | Acc: 0.6611 | F1: 0.7188 | ROC AUC: 0.7129 | PR AUC: 0.8085\n",
            "  Val   Loss: 0.1226 | Acc: 0.6432 | F1: 0.6675 | ROC AUC: 0.7332 | PR AUC: 0.7914\n",
            "Epoch 5/15 - Current learning rate: [0.0001, 0.001]\n",
            "Epoch 6/25\n",
            "  Train Loss: 0.4984 | Acc: 0.6541 | F1: 0.7143 | ROC AUC: 0.7070 | PR AUC: 0.8087\n",
            "  Val   Loss: 0.1247 | Acc: 0.6968 | F1: 0.7604 | ROC AUC: 0.7307 | PR AUC: 0.7908\n",
            "Epoch 6/15 - Current learning rate: [0.0001, 0.001]\n",
            "  Saved new best model with validation F1: 0.7604\n",
            "Epoch 7/25\n",
            "  Train Loss: 0.4955 | Acc: 0.6619 | F1: 0.7207 | ROC AUC: 0.7131 | PR AUC: 0.8096\n",
            "  Val   Loss: 0.1271 | Acc: 0.6914 | F1: 0.7666 | ROC AUC: 0.7221 | PR AUC: 0.7878\n",
            "Epoch 7/15 - Current learning rate: [0.0001, 0.001]\n",
            "  Saved new best model with validation F1: 0.7666\n",
            "Epoch 8/25\n",
            "  Train Loss: 0.4929 | Acc: 0.6683 | F1: 0.7275 | ROC AUC: 0.7183 | PR AUC: 0.8193\n",
            "  Val   Loss: 0.1234 | Acc: 0.6780 | F1: 0.7287 | ROC AUC: 0.7321 | PR AUC: 0.7885\n",
            "Epoch 8/15 - Current learning rate: [0.0001, 0.001]\n",
            "Epoch 9/25\n",
            "  Train Loss: 0.4904 | Acc: 0.6741 | F1: 0.7332 | ROC AUC: 0.7229 | PR AUC: 0.8228\n",
            "  Val   Loss: 0.1234 | Acc: 0.6473 | F1: 0.6658 | ROC AUC: 0.7283 | PR AUC: 0.7927\n",
            "Epoch 9/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 10/25\n",
            "  Train Loss: 0.4794 | Acc: 0.6785 | F1: 0.7354 | ROC AUC: 0.7390 | PR AUC: 0.8362\n",
            "  Val   Loss: 0.1234 | Acc: 0.6894 | F1: 0.7448 | ROC AUC: 0.7376 | PR AUC: 0.8000\n",
            "Epoch 10/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 11/25\n",
            "  Train Loss: 0.4833 | Acc: 0.6805 | F1: 0.7365 | ROC AUC: 0.7338 | PR AUC: 0.8287\n",
            "  Val   Loss: 0.1229 | Acc: 0.6861 | F1: 0.7367 | ROC AUC: 0.7372 | PR AUC: 0.7979\n",
            "Epoch 11/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 12/25\n",
            "  Train Loss: 0.4829 | Acc: 0.6772 | F1: 0.7346 | ROC AUC: 0.7346 | PR AUC: 0.8298\n",
            "  Val   Loss: 0.1239 | Acc: 0.6921 | F1: 0.7556 | ROC AUC: 0.7387 | PR AUC: 0.7990\n",
            "Epoch 12/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 13/25\n",
            "  Train Loss: 0.4782 | Acc: 0.6818 | F1: 0.7357 | ROC AUC: 0.7405 | PR AUC: 0.8357\n",
            "  Val   Loss: 0.1222 | Acc: 0.6854 | F1: 0.7368 | ROC AUC: 0.7394 | PR AUC: 0.7989\n",
            "Epoch 13/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 14/25\n",
            "  Train Loss: 0.4848 | Acc: 0.6802 | F1: 0.7373 | ROC AUC: 0.7314 | PR AUC: 0.8298\n",
            "  Val   Loss: 0.1226 | Acc: 0.6854 | F1: 0.7323 | ROC AUC: 0.7336 | PR AUC: 0.7961\n",
            "Epoch 14/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 15/25\n",
            "  Train Loss: 0.4867 | Acc: 0.6703 | F1: 0.7262 | ROC AUC: 0.7267 | PR AUC: 0.8254\n",
            "  Val   Loss: 0.1218 | Acc: 0.6660 | F1: 0.7063 | ROC AUC: 0.7364 | PR AUC: 0.7978\n",
            "Epoch 15/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 16/25\n",
            "  Train Loss: 0.4800 | Acc: 0.6775 | F1: 0.7341 | ROC AUC: 0.7361 | PR AUC: 0.8354\n",
            "  Val   Loss: 0.1233 | Acc: 0.6954 | F1: 0.7499 | ROC AUC: 0.7376 | PR AUC: 0.7965\n",
            "Epoch 16/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 17/25\n",
            "  Train Loss: 0.4770 | Acc: 0.6802 | F1: 0.7384 | ROC AUC: 0.7404 | PR AUC: 0.8351\n",
            "  Val   Loss: 0.1218 | Acc: 0.6707 | F1: 0.7140 | ROC AUC: 0.7360 | PR AUC: 0.7993\n",
            "Epoch 17/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 18/25\n",
            "  Train Loss: 0.4840 | Acc: 0.6711 | F1: 0.7279 | ROC AUC: 0.7292 | PR AUC: 0.8292\n",
            "  Val   Loss: 0.1223 | Acc: 0.6580 | F1: 0.6927 | ROC AUC: 0.7322 | PR AUC: 0.7930\n",
            "Epoch 18/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 19/25\n",
            "  Train Loss: 0.4766 | Acc: 0.6785 | F1: 0.7363 | ROC AUC: 0.7411 | PR AUC: 0.8380\n",
            "  Val   Loss: 0.1221 | Acc: 0.6760 | F1: 0.7212 | ROC AUC: 0.7383 | PR AUC: 0.7945\n",
            "Epoch 19/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 20/25\n",
            "  Train Loss: 0.4810 | Acc: 0.6782 | F1: 0.7350 | ROC AUC: 0.7363 | PR AUC: 0.8317\n",
            "  Val   Loss: 0.1227 | Acc: 0.6847 | F1: 0.7358 | ROC AUC: 0.7345 | PR AUC: 0.7922\n",
            "Epoch 20/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 21/25\n",
            "  Train Loss: 0.4778 | Acc: 0.6760 | F1: 0.7328 | ROC AUC: 0.7384 | PR AUC: 0.8348\n",
            "  Val   Loss: 0.1215 | Acc: 0.6740 | F1: 0.7144 | ROC AUC: 0.7385 | PR AUC: 0.7965\n",
            "Epoch 21/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 22/25\n",
            "  Train Loss: 0.4784 | Acc: 0.6802 | F1: 0.7351 | ROC AUC: 0.7415 | PR AUC: 0.8367\n",
            "  Val   Loss: 0.1219 | Acc: 0.6821 | F1: 0.7237 | ROC AUC: 0.7370 | PR AUC: 0.7956\n",
            "Epoch 22/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 23/25\n",
            "  Train Loss: 0.4758 | Acc: 0.6802 | F1: 0.7373 | ROC AUC: 0.7440 | PR AUC: 0.8370\n",
            "  Val   Loss: 0.1223 | Acc: 0.6834 | F1: 0.7255 | ROC AUC: 0.7375 | PR AUC: 0.7962\n",
            "Epoch 23/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 24/25\n",
            "  Train Loss: 0.4784 | Acc: 0.6830 | F1: 0.7396 | ROC AUC: 0.7425 | PR AUC: 0.8322\n",
            "  Val   Loss: 0.1223 | Acc: 0.6754 | F1: 0.7125 | ROC AUC: 0.7361 | PR AUC: 0.7967\n",
            "Epoch 24/15 - Current learning rate: [1e-05, 0.0001]\n",
            "Epoch 25/25\n",
            "  Train Loss: 0.4705 | Acc: 0.6862 | F1: 0.7422 | ROC AUC: 0.7499 | PR AUC: 0.8436\n",
            "  Val   Loss: 0.1215 | Acc: 0.6854 | F1: 0.7293 | ROC AUC: 0.7420 | PR AUC: 0.7995\n",
            "Epoch 25/15 - Current learning rate: [1e-05, 0.0001]\n",
            "\n",
            "Fold 1 Results:\n",
            "  accuracy: 0.6914\n",
            "  precision: 0.7169\n",
            "  recall: 0.8237\n",
            "  f1: 0.7666\n",
            "  roc_auc: 0.7221\n",
            "  pr_auc: 0.7878\n",
            "\n",
            "Training Fold 2/5\n",
            "Phase 1: Training only the MLP head...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ae22d64e68b3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_cross_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running 5-fold cross-validation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running standard training with 80/20 split...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0f82319be66d>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(reports_csv, proj_csv, image_folder, num_epochs, k)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Phase 1: Training only the MLP head...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc, train_pr_auc = train_epoch(\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-21-76ac259914cd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprobs_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Added for ROC AUC calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}